# IMPLEMENTACI√ìN DE UN MODELO DE INTELIGENCIA ARTIFICIAL PARA LA DETECCI√ìN DE VIOLENCIA EN AUDIO MEDIANTE AN√ÅLISIS AC√öSTICO Y PROCESAMIENTO DE LENGUAJE NATURAL

## Descripci√≥n del Proyecto

Este proyecto se centra en la **fragmentaci√≥n de audio en texto y espectrogramas** para el an√°lisis y predicci√≥n de violencia utilizando algoritmos de procesamiento de lenguaje natural (NLP). Combina modelos de aprendizaje profundo y t√©cnicas de NLP para identificar eventos violentos en grabaciones de audio segmentadas.

---

## üìö Contexto Acad√©mico

### FICHA RESUMEN

El presente trabajo de grado aborda el problema de identificaci√≥n, clasificaci√≥n y predicci√≥n de eventos violentos en registros de audio, mediante la implementaci√≥n de un modelo basado en t√©cnicas de procesamiento de lenguaje natural y aprendizaje profundo. El modelo desarrollado permite analizar fragmentos de audio segmentados, clasificando eventos en cinco categor√≠as espec√≠ficas: **disparos**, **gritos**, **conversaciones**, **llanto** y **rotura de vidrios**. A partir de esta clasificaci√≥n, se calcula un grado de violencia asociado a cada fragmento.

La soluci√≥n integra tres componentes principales:
- **Procesamiento de espectrogramas de audio** mediante redes neuronales profundas
- **An√°lisis del contenido textual** derivado de los audios utilizando representaciones vectoriales
- **Incremento del conjunto de datos** de entrenamiento mediante la incorporaci√≥n de ruido gaussiano para mejorar la capacidad de generalizaci√≥n del modelo

La metodolog√≠a empleada incluy√≥ la recopilaci√≥n y segmentaci√≥n de audios en intervalos de dos segundos, seguidos por el entrenamiento de ocho modelos diferentes: cuatro con datos originales y cuatro con datos aumentados con ruido, utilizando distintos valores de √©pocas (20, 30, 40 y 100). La clasificaci√≥n de violencia se organiza en tres niveles (**bajo**, **moderado** y **alto**), definidos seg√∫n el peso asignado a los eventos cr√≠ticos presentes en el audio.

Los resultados indican que los modelos entrenados con datos aumentados mostraron mejor capacidad de generalizaci√≥n en conjuntos de datos reducidos, especialmente en el experimento con 40 √©pocas. Asimismo, se verific√≥ la eficacia del modelo en escenarios simulados mediante la utilizaci√≥n de ejemplos extra√≠dos de pel√≠culas, alcanzando una detecci√≥n en tiempo casi real.

### ANTECEDENTES Y MOTIVACI√ìN

El estudio y an√°lisis de la voz humana constituye un campo ampliamente abordado dentro del √°mbito de la inteligencia artificial y el procesamiento digital de se√±ales. La violencia, en sus diversas manifestaciones, suele expresarse mediante indicadores ac√∫sticos tanto verbales como no verbales, los cuales evidencian estados de tensi√≥n, agresi√≥n o miedo.

**Aplicaciones Existentes:**
- **SoundThinking (2023)**: Soluci√≥n ShotSpotter que analiza en tiempo real sonidos de disparos, geolocalizando su procedencia
- **Modelos h√≠bridos**: Estudios como Trinh et al. (2020) han demostrado tasas de detecci√≥n superiores al 85% integrando espectrogramas de audio con an√°lisis sem√°ntico
- **Data augmentation**: Dur√£es, Veloso y Novais (2023) evaluaron t√©cnicas de aumento de datos para detectar eventos violentos

**Problem√°tica Actual:**
En contextos urbanos de Am√©rica Latina, como Bolivia, la proliferaci√≥n de contenido violento en entornos digitales se ha intensificado de manera alarmante. La ausencia de mecanismos automatizados para detectar, clasificar y valorar la gravedad de episodios violentos en se√±ales de audio limita la posibilidad de generar alertas tempranas y dificulta la intervenci√≥n oportuna.

### OBJETIVOS DEL PROYECTO

#### Objetivo General
Implementar un modelo basado en procesamiento de lenguaje natural y aprendizaje profundo para analizar se√±ales de audio mediante la segmentaci√≥n y conversi√≥n a espectrogramas, con el fin de predecir y clasificar episodios de violencia.

#### Objetivos Espec√≠ficos
- Segmentar las se√±ales de audio para generar representaciones textuales y en espectrogramas
- Preparar el conjunto de datos mediante su identificaci√≥n, limpieza y estructuraci√≥n para el entrenamiento del modelo de audio
- Dise√±ar un modelo de aprendizaje profundo, entrenarlo y evaluarlo para la clasificaci√≥n de contenido violento en se√±ales de audio
- Analizar el texto transcrito a partir del audio para identificar expresiones asociadas a violencia, mediante representaciones vectoriales
- Integrar las representaciones de audio y texto para predecir el grado de violencia en fragmentos de audio

### METODOLOG√çA

#### Arquitectura del Sistema
El modelo implementa una arquitectura h√≠brida que combina:

1. **Procesamiento Ac√∫stico**:
   - Segmentaci√≥n de audio en fragmentos de 2 segundos
   - Conversi√≥n a espectrogramas Mel con 128 bandas de frecuencia
   - Frecuencia de muestreo objetivo: 16kHz
   - Frecuencia m√°xima: 8kHz

2. **An√°lisis de Texto**:
   - Transcripci√≥n de audio a texto mediante Whisper-1 (OpenAI)
   - Generaci√≥n de embeddings vectoriales usando Sentence Transformers
   - B√∫squeda de similitud sem√°ntica en base de datos ChromaDB

3. **Clasificaci√≥n de Eventos**:
   - Redes neuronales convolucionales (CNN) para an√°lisis espectral
   - Clasificaci√≥n en 5 categor√≠as: crying, glass_breaking, gun_shot, people_talking, screams
   - C√°lculo de grados de violencia (bajo, moderado, alto)

#### Estrategia de Entrenamiento
- **8 modelos diferentes** entrenados con distintas configuraciones:
  - 4 modelos con dataset original (20, 30, 40, 100 √©pocas)
  - 4 modelos con data augmentation (ruido gaussiano + 20, 30, 40, 100 √©pocas)
- **T√©cnicas de data augmentation**: Ruido gaussiano para mejorar generalizaci√≥n
- **Validaci√≥n cruzada** para evaluar robustez del modelo

### RESULTADOS Y CONTRIBUCIONES

#### Rendimiento del Modelo
- **Mejor generalizaci√≥n**: Modelos con data augmentation mostraron superior capacidad de adaptaci√≥n
- **Punto √≥ptimo**: Experimento con 40 √©pocas y ruido gaussiano
- **Detecci√≥n en tiempo real**: Capacidad de procesamiento en tiempo casi real
- **Precisi√≥n**: Tasas de detecci√≥n superiores al 85% en escenarios controlados

#### Innovaciones T√©cnicas
- **Integraci√≥n multimodal**: Combinaci√≥n de an√°lisis ac√∫stico y sem√°ntico
- **Sistema de alertas**: Predicci√≥n de evoluci√≥n de violencia basada en tendencias
- **Base de datos vectorial**: Almacenamiento y b√∫squeda eficiente de embeddings
- **Interfaz intuitiva**: Aplicaci√≥n web con Streamlit para uso no t√©cnico

#### Aplicaciones Pr√°cticas
- **Monitoreo urbano**: Detecci√≥n de eventos violentos en espacios p√∫blicos
- **Seguridad ciudadana**: Alertas tempranas para cuerpos de seguridad
- **An√°lisis forense**: Clasificaci√≥n autom√°tica de evidencia de audio
- **Investigaci√≥n social**: Estudios sobre patrones de violencia en contextos urbanos

---

## Objetivo General

Fragmentaci√≥n de audio en texto y espectrograma para el an√°lisis y predicci√≥n de violencia mediante algoritmos de procesamiento de lenguaje natural.

---

## Objetivos Espec√≠ficos

- Descomposici√≥n de audio en texto y se√±ales de audio.
- Identificaci√≥n de set de datos para el entrenamiento y evaluaci√≥n del modelo de clasificaci√≥n.
- Creaci√≥n, entrenamiento y evaluaci√≥n del modelo.
- An√°lisis de texto usando embeddings.
- Identificaci√≥n de sonidos contextuales.
- Identificaci√≥n de falsos positivos y falsos negativos.
- Plan de contingencia y mejora del modelo para casos no identificados por el modelo.

### Caracter√≠sticas:
- **Subida de archivos de audio**: Permite cargar archivos de audio en formato WAV.
- **Segmentaci√≥n de audio**: Divide los archivos de audio en fragmentos de 2 segundos para su an√°lisis.
- **Clasificaci√≥n**: Detecta y clasifica eventos como llanto, vidrios rotos, disparos, gritos, y personas hablando.
- **Visualizaci√≥n**: Muestra gr√°ficos de la distribuci√≥n de las etiquetas y los eventos de alta probabilidad.
- **Hist√≥rico**: Los resultados de las predicciones se guardan en un archivo Excel para su posterior an√°lisis.

---

## üöÄ Instalaci√≥n y Configuraci√≥n

### Prerrequisitos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)
- Git (opcional, para clonar el repositorio)

### Pasos de Instalaci√≥n

1. **Clonar o descargar el proyecto**
   ```bash
   git clone <URL_DEL_REPOSITORIO>
   cd ViolenceDetectionInterface
   ```

2. **Crear un entorno virtual (recomendado)**
   ```bash
   # Crear entorno virtual
   python -m venv venv
   
   # Activar el entorno virtual
   # En Windows:
   venv\Scripts\activate
   # En macOS/Linux:
   source venv/bin/activate
   ```

3. **Instalar dependencias**
   ```bash
   pip install -r requirements.txt
   ```

4. **Configurar API Key de OpenAI (opcional)**
   
   Para usar la funcionalidad de transcripci√≥n de audio a texto, necesitas configurar tu API key de OpenAI:
   
   ```bash
   # En Windows:
   set OPENAI_API_KEY=tu_api_key_aqui
   
   # En macOS/Linux:
   export OPENAI_API_KEY=tu_api_key_aqui
   ```
   
   O crear un archivo `.env` en la ra√≠z del proyecto:
   ```
   OPENAI_API_KEY=tu_api_key_aqui
   ```

---

## üéØ C√≥mo Ejecutar la Aplicaci√≥n

### Ejecuci√≥n Principal

1. **Navegar al directorio del proyecto**
   ```bash
   cd project
   ```

2. **Ejecutar la aplicaci√≥n Streamlit**
   ```bash
   streamlit run main.py
   ```

3. **Acceder a la aplicaci√≥n**
   
   La aplicaci√≥n se abrir√° autom√°ticamente en tu navegador web. Si no se abre autom√°ticamente, ve a:
   ```
   http://localhost:8501
   ```

### Funcionalidades Disponibles

La aplicaci√≥n tiene tres p√°ginas principales:

#### 1. **Realizar Clasificaci√≥n de Audio** (`1_Realizar_clasificacion_de_audio.py`)
- Subir archivos de audio en formato WAV
- Seleccionar entre 8 modelos preentrenados diferentes
- Analizar fragmentos de audio de 2 segundos
- Obtener predicciones de eventos (llanto, disparos, vidrios rotos, etc.)
- Transcripci√≥n de audio a texto (requiere API key de OpenAI)
- B√∫squeda de similitudes en base de datos de embeddings
- Guardar resultados en archivo Excel

#### 2. **Ver Hist√≥rico** (`2_Ver_historico.py`)
- Visualizar archivos hist√≥ricos guardados
- Analizar resultados anteriores
- Descargar archivos de hist√≥rico

#### 3. **Determinar Grado de Violencia** (`3_Determinar_grado_de_violencia.py`)
- Calcular evoluci√≥n de violencia en el tiempo
- An√°lisis de tendencias de violencia
- Visualizaci√≥n de grados de violencia

---

## üìÅ Estructura del Proyecto

```
ViolenceDetectionInterface/
‚îÇ
‚îú‚îÄ‚îÄ models/                     # Modelos entrenados (.h5)
‚îÇ   ‚îú‚îÄ‚îÄ original_dataset_20.h5
‚îÇ   ‚îú‚îÄ‚îÄ original_dataset_30.h5
‚îÇ   ‚îú‚îÄ‚îÄ original_dataset_40.h5
‚îÇ   ‚îú‚îÄ‚îÄ original_dataset_100.h5
‚îÇ   ‚îú‚îÄ‚îÄ gaussian noise_20.h5
‚îÇ   ‚îú‚îÄ‚îÄ gaussian noise_30.h5
‚îÇ   ‚îú‚îÄ‚îÄ gaussian noise_40.h5
‚îÇ   ‚îî‚îÄ‚îÄ gaussian_noise_100.h5
‚îÇ
‚îú‚îÄ‚îÄ project/
‚îÇ   ‚îú‚îÄ‚îÄ modules/                # M√≥dulos auxiliares
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chroma_query.py     # Consultas a base de datos Chroma
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ audio_prediction.py # Predicci√≥n de audio
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ violence_grade.py   # C√°lculo de grado de violencia
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pages/                  # P√°ginas de la aplicaci√≥n
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 1_Realizar_clasificacion_de_audio.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 2_Ver_historico.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 3_Determinar_grado_de_violencia.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ main.py                 # Archivo principal de Streamlit
‚îÇ
‚îú‚îÄ‚îÄ hist√≥rico/                  # Archivos de resultados (.xlsx)
‚îú‚îÄ‚îÄ data_embeddings/           # Base de datos de embeddings
‚îú‚îÄ‚îÄ requirements.txt           # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md
```

---

## üîß Uso de la Aplicaci√≥n

### Paso a Paso para Clasificaci√≥n de Audio

1. **Abrir la aplicaci√≥n**
   - Ejecutar `streamlit run main.py` desde el directorio `project/`
   - Navegar a la p√°gina "Realizar Clasificaci√≥n de Audio"

2. **Seleccionar modelo**
   - Elegir entre los 8 modelos disponibles:
     - 4 modelos con audios originales (20, 30, 40, 100 epochs)
     - 4 modelos con ruido gaussiano (20, 30, 40, 100 epochs)

3. **Subir archivo de audio**
   - Hacer clic en "Browse files" o arrastrar archivo WAV
   - El sistema autom√°ticamente dividir√° el audio en fragmentos de 2 segundos

4. **Analizar resultados**
   - Ver predicciones para cada fragmento
   - Escuchar fragmentos individuales
   - Revisar transcripci√≥n de texto (si est√° configurada la API)
   - Ver embeddings asociados y distancias

5. **Descargar hist√≥rico**
   - Los resultados se guardan autom√°ticamente en formato Excel
   - El archivo se nombra seg√∫n el dataset y modelo seleccionado

### Modelos Disponibles

| Modelo | Descripci√≥n | √âpocas |
|--------|-------------|--------|
| Audios originales | Entrenado con dataset original | 20, 30, 40, 100 |
| Gaussian Noise | Entrenado con ruido gaussiano | 20, 30, 40, 100 |

---

## üêõ Soluci√≥n de Problemas

### Errores Comunes

1. **Error de dependencias**
   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt --force-reinstall
   ```

2. **Error de memoria con archivos grandes**
   - Usar archivos de audio m√°s peque√±os
   - Aumentar la memoria disponible

3. **Error de API de OpenAI**
   - Verificar que la API key est√© configurada correctamente
   - La funcionalidad de transcripci√≥n es opcional

4. **Error de puerto ocupado**
   ```bash
   streamlit run main.py --server.port 8502
   ```

### Requisitos del Sistema

- **RAM**: M√≠nimo 4GB, recomendado 8GB
- **Almacenamiento**: 2GB de espacio libre
- **Sistema Operativo**: Windows, macOS, o Linux

---

## üìä Formato de Salida

Los archivos de hist√≥rico se guardan en formato Excel (.xlsx) con las siguientes columnas:

- **N√∫mero de Fragmento**: Identificador del fragmento
- **Tiempo del Fragmento**: Rango de tiempo (ej: "0,2")
- **Texto**: Transcripci√≥n del audio (si est√° disponible)
- **Embedding Asociado**: Texto m√°s similar encontrado
- **Distancia**: Distancia de similitud
- **crying**: Probabilidad de llanto (%)
- **glass_breaking**: Probabilidad de vidrios rotos (%)
- **gun_shot**: Probabilidad de disparos (%)
- **people_talking**: Probabilidad de personas hablando (%)
- **screams**: Probabilidad de gritos (%)

---

## üìù Notas Importantes

- Los archivos de audio deben estar en formato WAV
- La transcripci√≥n de audio requiere una API key v√°lida de OpenAI
- Los modelos est√°n preentrenados y listos para usar
- Los resultados se guardan autom√°ticamente en la carpeta `hist√≥rico/`
- La aplicaci√≥n funciona mejor con archivos de audio de buena calidad

---

## Licencia

**Universidad Mayor de San Sim√≥n, Cochabamba, Bolivia**

El contenido de este proyecto es parte del trabajo de tesis titulado *"IMPLEMENTACI√ìN DE UN MODELO DE INTELIGENCIA ARTIFICIAL PARA LA DETECCI√ìN DE VIOLENCIA EN AUDIO MEDIANTE AN√ÅLISIS AC√öSTICO Y PROCESAMIENTO DE LENGUAJE NATURAL"*, desarrollado por **Carla Marcela Florida Roman** bajo la supervisi√≥n del **Tutor Eduardo Di Santi Gr√∂nros, PhD**.